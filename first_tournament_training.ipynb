{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tournament.agents.constant import AllC, AllD\n",
    "from tournament.agents.pavlov import Pavlov\n",
    "from tournament.agents.q_learning.dqn import DeepQLearner\n",
    "from tournament.agents.tft import TFTT, GenerousTFT, TitForTat\n",
    "from tournament.environments.single import SingleRuleBasedAgentEnvironment\n",
    "from tournament.environments.multiple import MultipleRuleBasedAgentEnvironment\n",
    "from tournament.action import Action\n",
    "from tournament.agents.agents import AGENTS\n",
    "from tournament.agents.axelrod_first import (\n",
    "    Davis,\n",
    "    Downing,\n",
    "    Feld,\n",
    "    Grofman,\n",
    "    Grudger,\n",
    "    Joss,\n",
    "    Nydegger,\n",
    "    Shubik,\n",
    "    SteinAndRapoport,\n",
    "    TidemanAndChieruzzi,\n",
    "    Tullock,\n",
    ")\n",
    "from tournament.agents.tft import (\n",
    "    TFTT,\n",
    "    TTFT,\n",
    "    GenerousTFT,\n",
    "    GradualTFT,\n",
    "    OmegaTFT,\n",
    "    TitForTat,\n",
    ")\n",
    "from tournament.tournament import RoundRobinTournament\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (20, 12)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "lb = [] # lookback\n",
    "eps = [] # epsilon\n",
    "eps_dcay = [] # epsilon decay\n",
    "dr = [] # discount rate\n",
    "nw = [] # network\n",
    "placement = [] # placement\n",
    "score_q_agent = [] # score of q agent\n",
    "final_loss = [] # final loss\n",
    "final_avg_reward = [] # final avg reward\n",
    "final_avg_rolling_reward = [] # final avg rolling reward last 20\n",
    "coop_percentage = [] # cooperaion percentage"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# env = SingleRuleBasedAgentEnvironment(TitForTat)\n",
    "env = MultipleRuleBasedAgentEnvironment([\n",
    "    TitForTat,\n",
    "    TidemanAndChieruzzi,\n",
    "    Nydegger,\n",
    "    Grofman,\n",
    "    Shubik,\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, lookback, n=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.elu = nn.ELU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(2 * lookback, n)\n",
    "        self.layer2 = nn.Linear(n, n)\n",
    "        # self.layer3 = nn.Linear(32, 32)\n",
    "        self.layer4 = nn.Linear(n, 2)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        # nn.init.kaiming_uniform_(self.layer3.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.layer4.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim=0)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        # x = torch.relu(self.layer3(x))\n",
    "        x = torch.relu(self.layer4(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLearningTest(DeepQLearner):\n",
    "\n",
    "    def __init__(self, lookback, epsilon, epsilon_decay, discount_rate, n):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.epsilon = epsilon\n",
    "        self._epsilon_decay = epsilon_decay\n",
    "        self._discount_rate = discount_rate\n",
    "        self._learning_rate = 0.01\n",
    "        self._q_network = QNetwork(self.lookback, n)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def one_train(a, b, c, d, e):\n",
    "    agent = QLearningTest(a, b, c, d, e)\n",
    "    env.train(\n",
    "        trainee=agent,\n",
    "        continuation_probability=1,\n",
    "        limit=200,\n",
    "        noise=0,\n",
    "        repetitions=1,\n",
    "        epochs=200,\n",
    "    )\n",
    "    tournament = RoundRobinTournament(AGENTS, [agent])\n",
    "\n",
    "    scores, times = tournament.play(\n",
    "        continuation_probability=0.99654, repetitions=50, jobs=4\n",
    "    )\n",
    "\n",
    "    results = [\n",
    "        (agent, round(sum(scores[agent]) / len(scores[agent])), sum(times[agent]))\n",
    "        for agent in scores\n",
    "    ]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    N = 20\n",
    "    # print(str(agent._q_network))\n",
    "    # print(\"loss: \"+str(env.metric_history[-1]))\n",
    "    # print(\"average reward: \"+str(np.mean(env.rewards)))\n",
    "    # print(\"rolling average reward: \"+str(np.convolve(env.rewards, np.ones(N), mode='valid')[-1] / N))\n",
    "    s = sum(env.counts.values())\n",
    "    ratio = {a: env.counts[a] / s for a in env.counts}\n",
    "    # print(\"cooperation ratio:\" + str(ratio.get(Action.COOPERATE)))\n",
    "    for i in results:\n",
    "        if i[0] == QLearningTest:\n",
    "            # print(\"placement:\" + str(results.index(i)))\n",
    "            placement.append(results.index(i))\n",
    "            # print(\"score:\" + str(i[1]))\n",
    "            score_q_agent.append(i[1])\n",
    "    lb.append(agent.lookback)\n",
    "    eps.append(agent.epsilon)\n",
    "    eps_dcay.append(agent._epsilon_decay)\n",
    "    dr.append(agent._discount_rate)\n",
    "    nw.append(agent._q_network)\n",
    "\n",
    "    final_loss.append(env.metric_history[-1])\n",
    "    final_avg_reward.append(np.mean(env.rewards))\n",
    "    final_avg_rolling_reward.append(np.convolve(env.rewards, np.ones(N), mode='valid')[-1] / N)\n",
    "    coop_percentage.append(ratio.get(Action.COOPERATE))\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lb_para = [1,2,4,6,8,10]\n",
    "eps_para = [0.05, 0.1, 0.15, 0.2]\n",
    "eps_dcay_para = [0.0, 0.002]\n",
    "dr_para = [0.95, 0.99]\n",
    "n_para = [4, 8, 16, 32, 64, 128]\n",
    "\n",
    "for e in n_para:\n",
    "    for d in dr_para:\n",
    "        for c in eps_dcay_para:\n",
    "            for b in eps_para:\n",
    "                for a in lb_para:\n",
    "                    one_train(a, b, c, d, e)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dic = {\n",
    "    \"lookback\": lb,\n",
    "    \"epsilon\": eps,\n",
    "    \"epsilon_decay\": eps_dcay,\n",
    "    \"discount_rate\": dr,\n",
    "    \"network\": nw,\n",
    "    \"placement\": placement,\n",
    "    \"score\": score_q_agent,\n",
    "    \"final_loss\": final_loss,\n",
    "    \"avg_reward\": final_avg_reward,\n",
    "    \"final_rolling_reward\": final_avg_rolling_reward,\n",
    "    \"coop_percentage\": coop_percentage\n",
    "}\n",
    "print(dic)\n",
    "\n",
    "df = pd.DataFrame(dic)\n",
    "df.to_csv('fisrt_tournament_result.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24a1ea954044c4f2d6e5c9c90b7867b351f6ffed6e2b222eec27b184b021ebe1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}